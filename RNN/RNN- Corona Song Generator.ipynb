{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN with tensorflow2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Be sure to used Tensorflow 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert hasattr(tf, \"function\") # Be sure to use tensorflow 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open and process dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27962\n",
      "CÃ©cile Ã©tait en blanc, comme aux tableaux illustres\n",
      "OÃ¹ la Sainte se voit, un nimbe autour du chef\n"
     ]
    }
   ],
   "source": [
    "# You can used your own dataset with english text\n",
    "\n",
    "with open(\"corona.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(len(text))\n",
    "\n",
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41 {'/', ',', '3', 'e', '9', 'x', ':', 'h', 'l', 'b', '.', 'p', '\\n', 'n', 'z', '\"', 'g', 'v', 'c', 't', ')', '|', 'j', 'q', 'i', 'm', '(', 'k', '0', 's', 'o', 'a', ' ', 'f', '%', \"'\", 'w', 'd', 'r', 'y', 'u'}\n",
      "ca(c)cile a(c)tait en blanc, comme aux tableaux illustres\n",
      "oa la sainte se voit, un nimbe autour du c\n"
     ]
    }
   ],
   "source": [
    "from unidecode import unidecode\n",
    "\n",
    "text = unidecode(text)\n",
    "text = text.lower()\n",
    "\n",
    "text = text.replace(\"2\", \"\")\n",
    "text = text.replace(\"1\", \"\")\n",
    "text = text.replace(\"8\", \"\")\n",
    "text = text.replace(\"5\", \"\")\n",
    "text = text.replace(\">\", \"\")\n",
    "text = text.replace(\"<\", \"\")\n",
    "text = text.replace(\"!\", \"\")\n",
    "text = text.replace(\"?\", \"\")\n",
    "text = text.replace(\"-\", \"\")\n",
    "text = text.replace(\"$\", \"\")\n",
    "text = text.replace(\";\", \"\")\n",
    "\n",
    "text = text.strip()\n",
    "\n",
    "vocab = set(text)\n",
    "print(len(vocab), vocab)\n",
    "\n",
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map each letter to int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_to_int {'/': 0, ',': 1, '3': 2, 'e': 3, '9': 4, 'x': 5, ':': 6, 'h': 7, 'l': 8, 'b': 9, '.': 10, 'p': 11, '\\n': 12, 'n': 13, 'z': 14, '\"': 15, 'g': 16, 'v': 17, 'c': 18, 't': 19, ')': 20, '|': 21, 'j': 22, 'q': 23, 'i': 24, 'm': 25, '(': 26, 'k': 27, '0': 28, 's': 29, 'o': 30, 'a': 31, ' ': 32, 'f': 33, '%': 34, \"'\": 35, 'w': 36, 'd': 37, 'r': 38, 'y': 39, 'u': 40}\n",
      "\n",
      "int_to_vocab {0: '/', 1: ',', 2: '3', 3: 'e', 4: '9', 5: 'x', 6: ':', 7: 'h', 8: 'l', 9: 'b', 10: '.', 11: 'p', 12: '\\n', 13: 'n', 14: 'z', 15: '\"', 16: 'g', 17: 'v', 18: 'c', 19: 't', 20: ')', 21: '|', 22: 'j', 23: 'q', 24: 'i', 25: 'm', 26: '(', 27: 'k', 28: '0', 29: 's', 30: 'o', 31: 'a', 32: ' ', 33: 'f', 34: '%', 35: \"'\", 36: 'w', 37: 'd', 38: 'r', 39: 'y', 40: 'u'}\n",
      "\n",
      "int for e: 3\n",
      "letter for 3: e\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocab)\n",
    "\n",
    "vocab_to_int = {l:i for i,l in enumerate(vocab)}\n",
    "int_to_vocab = {i:l for i,l in enumerate(vocab)}\n",
    "\n",
    "print(\"vocab_to_int\", vocab_to_int)\n",
    "print()\n",
    "print(\"int_to_vocab\", int_to_vocab)\n",
    "\n",
    "print(\"\\nint for e:\", vocab_to_int[\"e\"])\n",
    "int_for_e = vocab_to_int[\"e\"]\n",
    "print(\"letter for %s: %s\" % (vocab_to_int[\"e\"], int_to_vocab[int_for_e]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18, 31, 26, 18, 20, 18, 24, 8, 3, 32, 31, 26, 18, 20, 19, 31, 24, 19, 32, 3, 13, 32, 9, 8, 31, 13, 18, 1, 32, 18, 30, 25, 25, 3, 32, 31, 40, 5, 32, 19, 31, 9, 8, 3, 31, 40, 5, 32, 24, 8, 8, 40, 29, 19, 38, 3, 29, 12, 30, 31, 32, 8, 31, 32, 29, 31, 24, 13, 19, 3, 32, 29, 3, 32, 17, 30, 24, 19, 1, 32, 40, 13, 32, 13, 24, 25, 9, 3, 32, 31, 40, 19, 30, 40, 38, 32, 37, 40, 32, 18]\n"
     ]
    }
   ],
   "source": [
    "encoded = [vocab_to_int[l] for l in text]\n",
    "encoded_sentence = encoded[:100]\n",
    "\n",
    "print(encoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c', 'a', '(', 'c', ')', 'c', 'i', 'l', 'e', ' ', 'a', '(', 'c', ')', 't', 'a', 'i', 't', ' ', 'e', 'n', ' ', 'b', 'l', 'a', 'n', 'c', ',', ' ', 'c', 'o', 'm', 'm', 'e', ' ', 'a', 'u', 'x', ' ', 't', 'a', 'b', 'l', 'e', 'a', 'u', 'x', ' ', 'i', 'l', 'l', 'u', 's', 't', 'r', 'e', 's', '\\n', 'o', 'a', ' ', 'l', 'a', ' ', 's', 'a', 'i', 'n', 't', 'e', ' ', 's', 'e', ' ', 'v', 'o', 'i', 't', ',', ' ', 'u', 'n', ' ', 'n', 'i', 'm', 'b', 'e', ' ', 'a', 'u', 't', 'o', 'u', 'r', ' ', 'd', 'u', ' ', 'c']\n"
     ]
    }
   ],
   "source": [
    "decoded_sentence = [int_to_vocab[i] for i in encoded_sentence]\n",
    "print(decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ca(c)cile a(c)tait en blanc, comme aux tableaux illustres\n",
      "oa la sainte se voit, un nimbe autour du c\n"
     ]
    }
   ],
   "source": [
    "decoded_sentence = \"\".join(decoded_sentence)\n",
    "print(decoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample of one batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs [18, 31, 26, 18, 20, 18, 24, 8, 3, 32]\n",
      "Targets [31, 26, 18, 20, 18, 24, 8, 3, 32, 31]\n"
     ]
    }
   ],
   "source": [
    "inputs, targets = encoded, encoded[1:]\n",
    "\n",
    "print(\"Inputs\", inputs[:10])\n",
    "print(\"Targets\", targets[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method used to generate batch in sequence order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18. 31. 26. 18. 20.] [31. 26. 18. 20. 18.]\n",
      "[18. 31.  9.  9.  9.] [31. 26. 18. 20. 18.]\n"
     ]
    }
   ],
   "source": [
    "def gen_batch(inputs, targets, seq_len, batch_size, noise=0):\n",
    "    # Size of each chunk\n",
    "    chuck_size = (len(inputs) -1)  // batch_size\n",
    "    # Numbef of sequence per chunk\n",
    "    sequences_per_chunk = chuck_size // seq_len\n",
    "\n",
    "    for s in range(0, sequences_per_chunk):\n",
    "        batch_inputs = np.zeros((batch_size, seq_len))\n",
    "        batch_targets = np.zeros((batch_size, seq_len))\n",
    "        for b in range(0, batch_size):\n",
    "            fr = (b*chuck_size)+(s*seq_len)\n",
    "            to = fr+seq_len\n",
    "            batch_inputs[b] = inputs[fr:to]\n",
    "            batch_targets[b] = inputs[fr+1:to+1]\n",
    "            \n",
    "            if noise > 0:\n",
    "                noise_indices = np.random.choice(seq_len, noise)\n",
    "                batch_inputs[b][noise_indices] = np.random.randint(0, vocab_size)\n",
    "            \n",
    "        yield batch_inputs, batch_targets\n",
    "\n",
    "for batch_inputs, batch_targets in gen_batch(inputs, targets, 5, 32, noise=0):\n",
    "    print(batch_inputs[0], batch_targets[0])\n",
    "    break\n",
    "\n",
    "for batch_inputs, batch_targets in gen_batch(inputs, targets, 5, 32, noise=3):\n",
    "    print(batch_inputs[0], batch_targets[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your own layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneHot(tf.keras.layers.Layer):\n",
    "    def __init__(self, depth, **kwargs):\n",
    "        super(OneHot, self).__init__(**kwargs)\n",
    "        self.depth = depth\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        return tf.one_hot(tf.cast(x, tf.int32), self.depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 50)\n",
      "WARNING:tensorflow:Entity <bound method RnnModel.call of <__main__.RnnModel object at 0x000001DFC1782550>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method RnnModel.call of <__main__.RnnModel object at 0x000001DFC1782550>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method OneHot.call of <__main__.OneHot object at 0x000001DFC174CAC8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method OneHot.call of <__main__.OneHot object at 0x000001DFC174CAC8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
      "(32, 50, 41)\n",
      "Input letter is: 18.0\n",
      "One hot representation of the letter [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "class RnnModel(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super(RnnModel, self).__init__()\n",
    "        # Convolutions\n",
    "        self.one_hot = OneHot(len(vocab))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        output = self.one_hot(inputs)\n",
    "        return output\n",
    "\n",
    "batch_inputs, batch_targets = next(gen_batch(inputs, targets, 50, 32))\n",
    "\n",
    "print(batch_inputs.shape)\n",
    "\n",
    "model = RnnModel(len(vocab))\n",
    "output = model.predict(batch_inputs)\n",
    "\n",
    "print(output.shape)\n",
    "\n",
    "#print(output)\n",
    "\n",
    "print(\"Input letter is:\", batch_inputs[0][0])\n",
    "print(\"One hot representation of the letter\", output[0][0])\n",
    "\n",
    "#assert(output[int(batch_inputs[0][0])]==1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method OneHot.call of <__main__.OneHot object at 0x000001DFC18A4390>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method OneHot.call of <__main__.OneHot object at 0x000001DFC18A4390>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocab)\n",
    "\n",
    "### Creat the layers\n",
    "\n",
    "# Set the input of the model\n",
    "tf_inputs = tf.keras.Input(shape=(None,), batch_size=64)\n",
    "# Convert each value of the  input into a one encoding vector\n",
    "one_hot = OneHot(len(vocab))(tf_inputs)\n",
    "# Stack LSTM cells\n",
    "rnn_layer1 = tf.keras.layers.LSTM(128, return_sequences=True, stateful=True)(one_hot)\n",
    "rnn_layer2 = tf.keras.layers.LSTM(128, return_sequences=True, stateful=True)(rnn_layer1)\n",
    "# Create the outputs of the model\n",
    "hidden_layer = tf.keras.layers.Dense(128, activation=\"relu\")(rnn_layer2)\n",
    "outputs = tf.keras.layers.Dense(vocab_size, activation=\"softmax\")(hidden_layer)\n",
    "\n",
    "### Setup the model\n",
    "model = tf.keras.Model(inputs=tf_inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check if we can reset the RNN cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.reset_states()\n",
    "\n",
    "# Get one batch\n",
    "batch_inputs, batch_targets = next(gen_batch(inputs, targets, 50, 64))\n",
    "\n",
    "# Make a first prediction\n",
    "outputs = model.predict(batch_inputs)\n",
    "first_prediction = outputs[0][0]\n",
    "\n",
    "# Reset the states of the RNN states\n",
    "model.reset_states()\n",
    "\n",
    "# Make an other prediction to check the difference\n",
    "outputs = model.predict(batch_inputs)\n",
    "second_prediction = outputs[0][0]\n",
    "\n",
    "# Check if both prediction are equal\n",
    "assert(set(first_prediction)==set(second_prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set the loss and objectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam(lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set some metrics to track the progress of the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "# Accuracy\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set the train method and the predict method in graph mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inputs, targets):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Make a prediction on all the batch\n",
    "        predictions = model(inputs)\n",
    "        # Get the error/loss on these predictions\n",
    "        loss = loss_object(targets, predictions)\n",
    "    # Compute the gradient which respect to the loss\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    # Change the weights of the model\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    # The metrics are accumulate over time. You don't need to average it yourself.\n",
    "    train_loss(loss)\n",
    "    train_accuracy(targets, predictions)\n",
    "\n",
    "@tf.function\n",
    "def predict(inputs):\n",
    "    # Make a prediction on all the batch\n",
    "    predictions = model(inputs)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <function train_step at 0x000001DFC1CC5950> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Str'\n",
      "WARNING: Entity <function train_step at 0x000001DFC1CC5950> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Str'\n",
      "WARNING:tensorflow:Layer one_hot_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Entity <bound method OneHot.call of <__main__.OneHot object at 0x000001DFC18A4390>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method OneHot.call of <__main__.OneHot object at 0x000001DFC18A4390>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:From C:\\Users\\Amel\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1394: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x000001DFC17750D0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x000001DFC17750D0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method OneHot.call of <__main__.OneHot object at 0x000001DFC18A4390>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method OneHot.call of <__main__.OneHot object at 0x000001DFC18A4390>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
      " Epoch 239, Train Loss: 2.201303720474243, Train Accuracy: 34.5818023681640646"
     ]
    }
   ],
   "source": [
    "model.reset_states()\n",
    "\n",
    "for epoch in range(4000):\n",
    "    for batch_inputs, batch_targets in gen_batch(inputs, targets, 100, 64, noise=13):\n",
    "        train_step(batch_inputs, batch_targets)\n",
    "    template = '\\r Epoch {}, Train Loss: {}, Train Accuracy: {}'\n",
    "    print(template.format(epoch, train_loss.result(), train_accuracy.result()*100), end=\"\")\n",
    "    model.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# model.save(\"model_rnn.h5\")\n",
    "\n",
    "# with open(\"model_rnn_vocab_to_int\", \"w\") as f:\n",
    "    # f.write(json.dumps(vocab_to_int))\n",
    "# with open(\"model_rnn_int_to_vocab\", \"w\") as f:\n",
    "    #f.write(json.dumps(int_to_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "model.reset_states()\n",
    "\n",
    "size_poetries = 300\n",
    "\n",
    "poetries = np.zeros((64, size_poetries, 1))\n",
    "sequences = np.zeros((64, 100))\n",
    "for b in range(64):\n",
    "    rd = np.random.randint(0, len(inputs) - 100)\n",
    "    sequences[b] = inputs[rd:rd+100]\n",
    "\n",
    "for i in range(size_poetries+1):\n",
    "    if i > 0:\n",
    "        poetries[:,i-1,:] = sequences\n",
    "    softmax = predict(sequences)\n",
    "    # Set the next sequences\n",
    "    sequences = np.zeros((64, 1))\n",
    "    for b in range(64):\n",
    "        argsort = np.argsort(softmax[b][0])\n",
    "        argsort = argsort[::-1]\n",
    "        # Select one of the strongest 4 proposals\n",
    "        sequences[b] = argsort[0]\n",
    "\n",
    "for b in range(64):\n",
    "    sentence = \"\".join([int_to_vocab[i[0]] for i in poetries[b]])\n",
    "    print(sentence)\n",
    "    print(\"\\n=====================\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
